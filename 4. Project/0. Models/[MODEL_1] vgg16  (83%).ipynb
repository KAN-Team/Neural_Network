{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Step 1: Explore Data","metadata":{}},{"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Displaying Train Content\ndata_dir_list = os.listdir('../input/nn22-weather-analysis-using-image-recognition/Dataset/Train')\nprint(\"Train Directory Content: \", data_dir_list)\n\n# Displaying Test files\npath, dirs, files = next(os.walk('../input/nn22-weather-analysis-using-image-recognition/Dataset/Test'))\nfile_count = len(files)\nprint(\"Test Directory # of files: \", file_count)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:31:56.420387Z","iopub.execute_input":"2022-05-24T20:31:56.421110Z","iopub.status.idle":"2022-05-24T20:31:56.753684Z","shell.execute_reply.started":"2022-05-24T20:31:56.421015Z","shell.execute_reply":"2022-05-24T20:31:56.752948Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: Train-Valid Split","metadata":{}},{"cell_type":"markdown","source":"### Step 2.1: Establishing Working Directories","metadata":{}},{"cell_type":"code","source":"# making base directory for train-valid splitting\nbase_dir = './weather-data/'\nos.mkdir(base_dir)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:31:56.756542Z","iopub.execute_input":"2022-05-24T20:31:56.756734Z","iopub.status.idle":"2022-05-24T20:31:56.760691Z","shell.execute_reply.started":"2022-05-24T20:31:56.756711Z","shell.execute_reply":"2022-05-24T20:31:56.760025Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# creating two folders (train and valid)\ntrain_dir = os.path.join(base_dir, 'train')\nos.mkdir(train_dir)\n\nvalid_dir = os.path.join(base_dir, 'valid')\nos.mkdir(valid_dir)\n\n# creating eleven directories inside 'train' directory \n# ('rainbow', 'rain', 'hail', 'lightning', 'fogsmog', 'snow', 'glaze', 'rime', 'frost', 'sandstorm', 'dew')\n#0\ntrain_rainbow_dir = os.path.join(train_dir, 'rainbow')\nos.mkdir(train_rainbow_dir)\n#1\ntrain_rain_dir = os.path.join(train_dir, 'rain')\nos.mkdir(train_rain_dir)\n#2\ntrain_hail_dir = os.path.join(train_dir, 'hail')\nos.mkdir(train_hail_dir)\n#3\ntrain_lightning_dir = os.path.join(train_dir, 'lightning')\nos.mkdir(train_lightning_dir)\n#4\ntrain_fogsmog_dir = os.path.join(train_dir, 'fogsmog')\nos.mkdir(train_fogsmog_dir)\n#5\ntrain_snow_dir = os.path.join(train_dir, 'snow')\nos.mkdir(train_snow_dir)\n#6\ntrain_glaze_dir = os.path.join(train_dir, 'glaze')\nos.mkdir(train_glaze_dir)\n#7\ntrain_rime_dir = os.path.join(train_dir, 'rime')\nos.mkdir(train_rime_dir)\n#8\ntrain_frost_dir = os.path.join(train_dir, 'frost')\nos.mkdir(train_frost_dir)\n#9\ntrain_sandstorm_dir = os.path.join(train_dir, 'sandstorm')\nos.mkdir(train_sandstorm_dir)\n#10\ntrain_dew_dir = os.path.join(train_dir, 'dew')\nos.mkdir(train_dew_dir)\n\n# creating eleven directories inside 'valid' directory \n# ('rainbow', 'rain', 'hail', 'lightning', 'fogsmog', 'snow', 'glaze', 'rime', 'frost', 'sandstorm', 'dew')\n#0\nvalid_rainbow_dir = os.path.join(valid_dir, 'rainbow')\nos.mkdir(valid_rainbow_dir)\n#1\nvalid_rain_dir = os.path.join(valid_dir, 'rain')\nos.mkdir(valid_rain_dir)\n#2\nvalid_hail_dir = os.path.join(valid_dir, 'hail')\nos.mkdir(valid_hail_dir)\n#3\nvalid_lightning_dir = os.path.join(valid_dir, 'lightning')\nos.mkdir(valid_lightning_dir)\n#4\nvalid_fogsmog_dir = os.path.join(valid_dir, 'fogsmog')\nos.mkdir(valid_fogsmog_dir)\n#5\nvalid_snow_dir = os.path.join(valid_dir, 'snow')\nos.mkdir(valid_snow_dir)\n#6\nvalid_glaze_dir = os.path.join(valid_dir, 'glaze')\nos.mkdir(valid_glaze_dir)\n#7\nvalid_rime_dir = os.path.join(valid_dir, 'rime')\nos.mkdir(valid_rime_dir)\n#8\nvalid_frost_dir = os.path.join(valid_dir, 'frost')\nos.mkdir(valid_frost_dir)\n#9\nvalid_sandstorm_dir = os.path.join(valid_dir, 'sandstorm')\nos.mkdir(valid_sandstorm_dir)\n#10\nvalid_dew_dir = os.path.join(valid_dir, 'dew')\nos.mkdir(valid_dew_dir)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:31:56.962179Z","iopub.execute_input":"2022-05-24T20:31:56.962724Z","iopub.status.idle":"2022-05-24T20:31:56.980962Z","shell.execute_reply.started":"2022-05-24T20:31:56.962689Z","shell.execute_reply":"2022-05-24T20:31:56.980246Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import random\nfrom shutil import copyfile\n\ndef train_valid_split(src_dir, train_dir, valid_dir, split_size):\n    files = []\n    for filename in os.listdir(src_dir):\n        file = os.path.join(src_dir, filename)\n        if os.path.getsize(file) > 0:\n            files.append(filename)\n        else:\n            print(filename + \"is ignored as it cannot be fetched!!\")\n\n    # shuffling train data and splitting into train and valid sets\n    train_size = int(len(files) * split_size)\n    valid_size = int(len(files) - train_size)\n    shuffled_set = random.sample(files, len(files))\n    train_set = shuffled_set[0:train_size]\n    valid_set = shuffled_set[train_size:]\n    \n    # copying train set files from source to destination \n    for filename in train_set:\n        curr_file = os.path.join(src_dir, filename)\n        move_dest = os.path.join(train_dir, filename)\n        copyfile(curr_file, move_dest)\n    \n    # copying valid set files from source to destination \n    for filename in valid_set:\n        curr_file = os.path.join(src_dir, filename)\n        move_dest = os.path.join(valid_dir, filename)\n        copyfile(curr_file, move_dest)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:31:57.255187Z","iopub.execute_input":"2022-05-24T20:31:57.255599Z","iopub.status.idle":"2022-05-24T20:31:57.268904Z","shell.execute_reply.started":"2022-05-24T20:31:57.255565Z","shell.execute_reply":"2022-05-24T20:31:57.267670Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\n\nTRAIN_SRC_DIR = '../input/nn22-weather-analysis-using-image-recognition/Dataset/Train/'\nTRAIN_DIR = './weather-data/train/'\nVALID_DIR = './weather-data/valid/'\n\n#0\nDEW_SOURCE_DIR = os.path.join(TRAIN_SRC_DIR, 'dew')\nDEW_TRAIN_DIR = os.path.join(TRAIN_DIR, 'dew')\nDEW_VALID_DIR = os.path.join(VALID_DIR, 'dew')\n\n#1\nFOGSMOG_SOURCE_DIR = os.path.join(TRAIN_SRC_DIR, 'fogsmog')\nFOGSMOG_TRAIN_DIR = os.path.join(TRAIN_DIR, 'fogsmog')\nFOGSMOG_VALID_DIR = os.path.join(VALID_DIR, 'fogsmog')\n\n#2\nFROST_SOURCE_DIR = os.path.join(TRAIN_SRC_DIR, 'frost')\nFROST_TRAIN_DIR = os.path.join(TRAIN_DIR, 'frost')\nFROST_VALID_DIR = os.path.join(VALID_DIR, 'frost')\n\n#3\nGLAZE_SOURCE_DIR = os.path.join(TRAIN_SRC_DIR, 'glaze')\nGLAZE_TRAIN_DIR = os.path.join(TRAIN_DIR, 'glaze')\nGLAZE_VALID_DIR = os.path.join(VALID_DIR, 'glaze')\n\n#4\nHAIL_SOURCE_DIR = os.path.join(TRAIN_SRC_DIR, 'hail')\nHAIL_TRAIN_DIR = os.path.join(TRAIN_DIR, 'hail')\nHAIL_VALID_DIR = os.path.join(VALID_DIR, 'hail')\n\n#5\nLIGHTNING_SOURCE_DIR = os.path.join(TRAIN_SRC_DIR, 'lightning')\nLIGHTNING_TRAIN_DIR = os.path.join(TRAIN_DIR, 'lightning')\nLIGHTNING_VALID_DIR = os.path.join(VALID_DIR, 'lightning')\n\n#6\nRAIN_SOURCE_DIR = os.path.join(TRAIN_SRC_DIR, 'rain')\nRAIN_TRAIN_DIR = os.path.join(TRAIN_DIR, 'rain')\nRAIN_VALID_DIR = os.path.join(VALID_DIR, 'rain')\n\n#7\nRAINBOW_SOURCE_DIR = os.path.join(TRAIN_SRC_DIR, 'rainbow')\nRAINBOW_TRAIN_DIR = os.path.join(TRAIN_DIR, 'rainbow')\nRAINBOW_VALID_DIR = os.path.join(VALID_DIR, 'rainbow')\n\n#8\nRIME_SOURCE_DIR = os.path.join(TRAIN_SRC_DIR, 'rime')\nRIME_TRAIN_DIR = os.path.join(TRAIN_DIR, 'rime')\nRIME_VALID_DIR = os.path.join(VALID_DIR, 'rime')\n\n#9\nSANDSTORM_SOURCE_DIR = os.path.join(TRAIN_SRC_DIR, 'sandstorm')\nSANDSTORM_TRAIN_DIR = os.path.join(TRAIN_DIR, 'sandstorm')\nSANDSTORM_VALID_DIR = os.path.join(VALID_DIR, 'sandstorm')\n\n#10\nSNOW_SOURCE_DIR = os.path.join(TRAIN_SRC_DIR, 'snow')\nSNOW_TRAIN_DIR = os.path.join(TRAIN_DIR, 'snow')\nSNOW_VALID_DIR = os.path.join(VALID_DIR, 'snow')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:31:57.459775Z","iopub.execute_input":"2022-05-24T20:31:57.460037Z","iopub.status.idle":"2022-05-24T20:31:57.478669Z","shell.execute_reply.started":"2022-05-24T20:31:57.460009Z","shell.execute_reply":"2022-05-24T20:31:57.477782Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"split_portion = .90\n\n# splitting each class into train-validation sets\ntrain_valid_split(DEW_SOURCE_DIR, DEW_TRAIN_DIR, DEW_VALID_DIR, split_portion)\ntrain_valid_split(FOGSMOG_SOURCE_DIR, FOGSMOG_TRAIN_DIR, FOGSMOG_VALID_DIR, split_portion)\ntrain_valid_split(FROST_SOURCE_DIR, FROST_TRAIN_DIR, FROST_VALID_DIR, split_portion)\ntrain_valid_split(GLAZE_SOURCE_DIR, GLAZE_TRAIN_DIR, GLAZE_VALID_DIR, split_portion)\ntrain_valid_split(HAIL_SOURCE_DIR, HAIL_TRAIN_DIR, HAIL_VALID_DIR, split_portion)\ntrain_valid_split(LIGHTNING_SOURCE_DIR, LIGHTNING_TRAIN_DIR, LIGHTNING_VALID_DIR, split_portion)\ntrain_valid_split(RAIN_SOURCE_DIR, RAIN_TRAIN_DIR, RAIN_VALID_DIR, split_portion)\ntrain_valid_split(RAINBOW_SOURCE_DIR, RAINBOW_TRAIN_DIR, RAINBOW_VALID_DIR, split_portion)\ntrain_valid_split(RIME_SOURCE_DIR, RIME_TRAIN_DIR, RIME_VALID_DIR, split_portion)\ntrain_valid_split(SANDSTORM_SOURCE_DIR, SANDSTORM_TRAIN_DIR, SANDSTORM_VALID_DIR, split_portion)\ntrain_valid_split(SNOW_SOURCE_DIR, SNOW_TRAIN_DIR, SNOW_VALID_DIR, split_portion)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:31:57.775734Z","iopub.execute_input":"2022-05-24T20:31:57.776237Z","iopub.status.idle":"2022-05-24T20:32:28.131970Z","shell.execute_reply.started":"2022-05-24T20:31:57.776186Z","shell.execute_reply":"2022-05-24T20:32:28.131243Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Step 2.2: Train-Valid Analysis","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.image import imread\nimport pathlib\n\ndef plot_classes_distributions(dir_path, plot_title):\n    class_imgs_num = {}\n    for lbl in class_labels:\n        class_imgs_num[lbl] = len(os.listdir(dir_path+lbl+'/'))\n\n    plt.figure(figsize=(9, 6))\n    plt.bar(range(len(class_imgs_num)), list(class_imgs_num.values()), align='center')\n    plt.xticks(range(len(class_imgs_num)), list(class_imgs_num.keys()))\n    plt.title(plot_title)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:32:28.134534Z","iopub.execute_input":"2022-05-24T20:32:28.135000Z","iopub.status.idle":"2022-05-24T20:32:29.004921Z","shell.execute_reply.started":"2022-05-24T20:32:28.134961Z","shell.execute_reply":"2022-05-24T20:32:29.004241Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"plot_classes_distributions(TRAIN_DIR, 'Distribution of all classes in the Train Set')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:32:29.006251Z","iopub.execute_input":"2022-05-24T20:32:29.006481Z","iopub.status.idle":"2022-05-24T20:32:29.219212Z","shell.execute_reply.started":"2022-05-24T20:32:29.006450Z","shell.execute_reply":"2022-05-24T20:32:29.218548Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"plot_classes_distributions(VALID_DIR, 'Distribution of all classes in the Valid Set')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:32:29.221001Z","iopub.execute_input":"2022-05-24T20:32:29.222338Z","iopub.status.idle":"2022-05-24T20:32:29.404727Z","shell.execute_reply.started":"2022-05-24T20:32:29.222298Z","shell.execute_reply":"2022-05-24T20:32:29.404100Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"for lbl in class_labels:\n    print('# of Train {} images are: '.format(lbl) + str(len(os.listdir(TRAIN_DIR + lbl +'/'))))\n    \nprint('========================================')\n\nfor lbl in class_labels:\n    print('# of Valid {} images are: '.format(lbl) + str(len(os.listdir(VALID_DIR + lbl +'/'))))","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:32:29.406047Z","iopub.execute_input":"2022-05-24T20:32:29.406313Z","iopub.status.idle":"2022-05-24T20:32:29.421778Z","shell.execute_reply.started":"2022-05-24T20:32:29.406280Z","shell.execute_reply":"2022-05-24T20:32:29.421162Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Deep Learning with Data Augmentation","metadata":{}},{"cell_type":"markdown","source":"### Step 3.1: Image Data Generator","metadata":{}},{"cell_type":"code","source":"IMG_WIDTH = 224\nIMG_HEIGHT = 224\nBATCH_SIZE = 16\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ntrain_datagen = ImageDataGenerator(rescale = 1/255.0,\n                                   rotation_range=30,\n                                   zoom_range=0.4,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   shear_range=0.15,\n                                   horizontal_flip=True)\n\ntrain_generator = train_datagen.flow_from_directory(TRAIN_DIR,\n                                                    batch_size=BATCH_SIZE,\n                                                    class_mode='categorical',\n                                                    target_size=(IMG_HEIGHT, IMG_WIDTH))","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:32:29.423034Z","iopub.execute_input":"2022-05-24T20:32:29.423292Z","iopub.status.idle":"2022-05-24T20:32:35.244380Z","shell.execute_reply.started":"2022-05-24T20:32:29.423259Z","shell.execute_reply":"2022-05-24T20:32:35.243579Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"valid_datagen = ImageDataGenerator(rescale = 1/255.0)\n\nvalid_generator = valid_datagen.flow_from_directory(VALID_DIR,\n                                                    batch_size=BATCH_SIZE,\n                                                    class_mode='categorical',\n                                                    target_size=(IMG_HEIGHT, IMG_WIDTH))","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:32:35.245723Z","iopub.execute_input":"2022-05-24T20:32:35.246151Z","iopub.status.idle":"2022-05-24T20:32:35.353914Z","shell.execute_reply.started":"2022-05-24T20:32:35.246112Z","shell.execute_reply":"2022-05-24T20:32:35.353240Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Step 3.2: Building The Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, MaxPool2D, Activation\n\ndef VGG16(W_IMG=IMG_WIDTH, H_IMG=IMG_HEIGHT, N_CLASSES=11):\n    model = Sequential()\n    model.add(Conv2D(input_shape=(W_IMG, H_IMG, 3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2),name='vgg16'))\n    model.add(Flatten())\n    model.add(Dropout(0.3))\n    model.add(Dense(768, activation='relu', name='fc1'))\n    model.add(Dropout(0.25))\n    model.add(Dense(256, activation='relu', name='fc2'))\n    model.add(Dropout(0.25))\n    model.add(Dense(N_CLASSES, activation='softmax', name='output'))\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-24T22:27:02.986551Z","iopub.execute_input":"2022-05-24T22:27:02.986809Z","iopub.status.idle":"2022-05-24T22:27:03.004214Z","shell.execute_reply.started":"2022-05-24T22:27:02.986781Z","shell.execute_reply":"2022-05-24T22:27:03.003506Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nmodel = VGG16()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T22:27:05.403182Z","iopub.execute_input":"2022-05-24T22:27:05.403862Z","iopub.status.idle":"2022-05-24T22:27:05.556437Z","shell.execute_reply.started":"2022-05-24T22:27:05.403827Z","shell.execute_reply":"2022-05-24T22:27:05.555760Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nVgg16 = Model(inputs=model.input, outputs=model.get_layer('vgg16').output)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T22:27:10.442795Z","iopub.execute_input":"2022-05-24T22:27:10.443052Z","iopub.status.idle":"2022-05-24T22:27:10.451671Z","shell.execute_reply.started":"2022-05-24T22:27:10.443024Z","shell.execute_reply":"2022-05-24T22:27:10.450593Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n# Load weights\nWEIGHTS_PATH_NO_TOP = ('https://github.com/fchollet/deep-learning-models/'\n                       'releases/download/v0.1/'\n                       'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')\n\nweights_path = tf.keras.utils.get_file(\n    'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n    WEIGHTS_PATH_NO_TOP,\n    cache_subdir='models',\n    file_hash='6d6bbae143d832006294945121d1f1fc')\n\nVgg16.load_weights(weights_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T22:27:12.958958Z","iopub.execute_input":"2022-05-24T22:27:12.959497Z","iopub.status.idle":"2022-05-24T22:27:13.127589Z","shell.execute_reply.started":"2022-05-24T22:27:12.959460Z","shell.execute_reply":"2022-05-24T22:27:13.126871Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"### Freezing the Weights ###\nfor layer in Vgg16.layers:\n    layer.trainable = False\n\nfor layer in model.layers:\n    print(layer, layer.trainable)\n    \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T22:27:19.066623Z","iopub.execute_input":"2022-05-24T22:27:19.067186Z","iopub.status.idle":"2022-05-24T22:27:19.092523Z","shell.execute_reply.started":"2022-05-24T22:27:19.067148Z","shell.execute_reply":"2022-05-24T22:27:19.091848Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"### Step 3.3: Model Optimizer","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.optimizers import SGD\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=1e-4), metrics=[\"accuracy\"])\n# model.compile(loss=\"binary_crossentropy\", optimizer=SGD(lr=1e-4, momentum=0.9), metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-24T22:27:31.665313Z","iopub.execute_input":"2022-05-24T22:27:31.665611Z","iopub.status.idle":"2022-05-24T22:27:31.686165Z","shell.execute_reply.started":"2022-05-24T22:27:31.665577Z","shell.execute_reply":"2022-05-24T22:27:31.685287Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"### Step 3.4: Model Checkpoint & EarlyStopping Callback","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n# autosave best Model\ncheckpoint_path = 'training_1/vgg16_best_model.ckpt'\ncheckpoint_dir = os.path.dirname(checkpoint_path)\nearly = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=5)\ncheckpoint = ModelCheckpoint(checkpoint_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T23:42:45.267106Z","iopub.execute_input":"2022-05-24T23:42:45.267771Z","iopub.status.idle":"2022-05-24T23:42:45.273238Z","shell.execute_reply.started":"2022-05-24T23:42:45.267734Z","shell.execute_reply":"2022-05-24T23:42:45.272045Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"### Step 3.5: Model Training","metadata":{}},{"cell_type":"code","source":"history = model.fit_generator(train_generator,\n                              epochs=20,\n                              verbose=1,\n                              validation_data=valid_generator,\n                              callbacks = [checkpoint, early])","metadata":{"execution":{"iopub.status.busy":"2022-05-24T23:42:47.859307Z","iopub.execute_input":"2022-05-24T23:42:47.859566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: Model Evaluation","metadata":{}},{"cell_type":"code","source":"evl = model.evaluate_generator(valid_generator)\nprint(\"Loss: {:0.4f}\".format(evl[0]), \"Score: {:0.4f}\".format(evl[1]))","metadata":{"execution":{"iopub.status.busy":"2022-05-24T23:39:10.026566Z","iopub.execute_input":"2022-05-24T23:39:10.026849Z","iopub.status.idle":"2022-05-24T23:39:14.311520Z","shell.execute_reply.started":"2022-05-24T23:39:10.026818Z","shell.execute_reply":"2022-05-24T23:39:14.310765Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"### Step 4.1: Performance Visusalization","metadata":{}},{"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs=range(len(acc))\n\nimport matplotlib.pyplot as plt\nplt.plot(epochs, acc, 'r', label=\"Training Accuracy\")\nplt.plot(epochs, val_acc, 'b', label=\"Validation Accuracy\")\nplt.plot(epochs, loss, 'r', label=\"Training Loss\")\nplt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\nplt.title(\"Model Accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T23:39:16.487108Z","iopub.execute_input":"2022-05-24T23:39:16.487790Z","iopub.status.idle":"2022-05-24T23:39:16.692398Z","shell.execute_reply.started":"2022-05-24T23:39:16.487752Z","shell.execute_reply":"2022-05-24T23:39:16.691729Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Model Prediction","metadata":{}},{"cell_type":"markdown","source":"### Step 5.1: Reading Test Images into a Dataframe","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nTEST_DIR = '../input/nn22-weather-analysis-using-image-recognition/Dataset/Test'\nimg_path = os.listdir(TEST_DIR)\ntest_df = pd.DataFrame({'image_name': img_path})\nn_test_samples = test_df.shape[0]\nprint(\"Number of Loaded Test Data Samples: \", n_test_samples)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T23:39:29.310186Z","iopub.execute_input":"2022-05-24T23:39:29.310484Z","iopub.status.idle":"2022-05-24T23:39:29.328124Z","shell.execute_reply.started":"2022-05-24T23:39:29.310454Z","shell.execute_reply":"2022-05-24T23:39:29.327460Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"### Step 5.2: Test Image Data Generator","metadata":{}},{"cell_type":"code","source":"test_datagen = ImageDataGenerator(rescale = 1/255.0)\n\ntest_generator = test_datagen.flow_from_dataframe(test_df, \n                                                  directory=TEST_DIR,\n                                                  x_col='image_name',\n                                                  target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                  y_col=None,\n                                                  batch_size=1,\n                                                  class_mode=None,\n                                                  shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T23:39:36.860414Z","iopub.execute_input":"2022-05-24T23:39:36.860667Z","iopub.status.idle":"2022-05-24T23:39:37.416651Z","shell.execute_reply.started":"2022-05-24T23:39:36.860640Z","shell.execute_reply":"2022-05-24T23:39:37.415663Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\npred_array = model.predict(test_generator, steps=np.ceil(n_test_samples/1.0))\npredictions = np.argmax(pred_array, axis=1)\ntest_df['label'] = predictions\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T23:40:13.980129Z","iopub.execute_input":"2022-05-24T23:40:13.980844Z","iopub.status.idle":"2022-05-24T23:40:24.942714Z","shell.execute_reply.started":"2022-05-24T23:40:13.980797Z","shell.execute_reply":"2022-05-24T23:40:24.942032Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"## Step 6: Model Saving","metadata":{}},{"cell_type":"code","source":"model.save('vgg16-model')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T23:41:22.502304Z","iopub.execute_input":"2022-05-24T23:41:22.503021Z","iopub.status.idle":"2022-05-24T23:41:25.757699Z","shell.execute_reply.started":"2022-05-24T23:41:22.502984Z","shell.execute_reply":"2022-05-24T23:41:25.756858Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"## Step 7: Submission File Creation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndef create_submission_file(X_test, Y_test):\n    df = pd.DataFrame({'image_name': X_test,'label': Y_test})\n    df.to_csv(r'./CS_22_submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T23:41:50.493097Z","iopub.execute_input":"2022-05-24T23:41:50.493785Z","iopub.status.idle":"2022-05-24T23:41:50.498276Z","shell.execute_reply.started":"2022-05-24T23:41:50.493750Z","shell.execute_reply":"2022-05-24T23:41:50.497253Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"print(\"[START]: File Creation...\")\ntest_df.to_csv(r'./submission_1.csv', index=False)\n# create_submission_file(X_test, Y_test)\nprint(\"[END]: File Creation!\")","metadata":{"execution":{"iopub.status.busy":"2022-05-24T23:42:00.322796Z","iopub.execute_input":"2022-05-24T23:42:00.323053Z","iopub.status.idle":"2022-05-24T23:42:00.335590Z","shell.execute_reply.started":"2022-05-24T23:42:00.323024Z","shell.execute_reply":"2022-05-24T23:42:00.334718Z"},"trusted":true},"execution_count":67,"outputs":[]}]}